{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "rqzwHrIw-raW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "my_seed = 23753\n",
        "np.random.seed(my_seed)\n",
        "tf.random.set_seed(my_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "1c1ibB96-zIn"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# To use fashion_mnist uncomment the next two lines.\n",
        "# from keras.datasets import fashion_mnist\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIK_hbE-3hh",
        "outputId": "92f2d149-fa60-4eb4-9fb2-89347316a835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dimension:\n",
            "(60000, 784)\n",
            "Test dimension:\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# normalize x\n",
        "X_train = X_train.astype(float) / 255.\n",
        "X_test = X_test.astype(float) / 255.\n",
        "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n",
        "print('Train dimension:');print(X_train.shape)\n",
        "print('Test dimension:');print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-RcAdbz_rWX",
        "outputId": "3d40fc4a-b233-438c-a1c8-cab65a8ccf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels dimension:\n",
            "(60000, 10)\n",
            "Test labels dimension:\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "# Define the One-hot Encoder\n",
        "ohe = preprocessing.OneHotEncoder()\n",
        "# Reshape data\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Fit and transform training data\n",
        "ohe.fit(y_train)\n",
        "y_train = ohe.transform(y_train).toarray()\n",
        "\n",
        "# Fit and transform testing data\n",
        "ohe.fit(y_test)\n",
        "y_test = ohe.transform(y_test).toarray()\n",
        "\n",
        "print('Train labels dimension:');print(y_train.shape)\n",
        "print('Test labels dimension:');print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "CwNtnyZS_83q"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden1 = 512\n",
        "size_hidden2 = 256\n",
        "size_output = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "42wLFJrd__rM"
      },
      "outputs": [],
      "source": [
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(128)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "unf5qVHCAMWu"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of hidden layer 1\n",
        "    size_hidden2: int, size of hidden layer 2\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_output, device\n",
        "    \n",
        "    # # Initialize weights between input layer and hidden layer\n",
        "    # self.W0 = tf.Variable(tf.random.normal([self.size_input, self.size_input_layer]))\n",
        "    # # Initialize biases for hidden layer\n",
        "    # self.b0 = tf.Variable(tf.random.normal([1, self.size_input_layer]))\n",
        "\n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "\n",
        "    # Initialize weights between hidden layer and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output]))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    #y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    loss = cce(y_true_tf, y_pred_tf)\n",
        "    l2_penalty = 0.001 * ((tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2)) + tf.reduce_sum(tf.square(self.W3)))/3)\n",
        "    #Uncomment to add L2 Penalty\n",
        "    #return loss + l2_penalty\n",
        "    return loss\n",
        "\n",
        "  def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=6e-2)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    keep_prob = 0.3\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #Remember to normalize your dataset before moving forward\n",
        "    \n",
        "    # # Compute values in input layer\n",
        "    # what0 = tf.matmul(X_tf, self.W0) + self.b0\n",
        "    # hhat0 = tf.nn.relu(what0)\n",
        "    # hhat00 = tf.nn.dropout(hhat0, keep_prob)\n",
        "\n",
        "    # Compute values in hidden layer 1\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    #Uncomment to add dropout\n",
        "    #hhat11 = tf.nn.dropout(hhat1, keep_prob)\n",
        "    hhat11 = hhat1\n",
        "\n",
        "    # Compute values in hidden layer 2\n",
        "    what2 = tf.matmul(hhat11, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    #Uncomment to add dropout\n",
        "    #hhat22 = tf.nn.dropout(hhat2, keep_prob)\n",
        "    hhat22 = hhat2\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat22, self.W3) + self.b3\n",
        "    op = tf.nn.softmax(output)\n",
        "\n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
        "    #Second add tf.Softmax(output) and then return this variable\n",
        "    return op\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(tf.reshape(y_pred, (-1, self.size_output)), dtype=tf.float32)\n",
        "    correct_pred = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_pred, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "NX6Ut_PsBgjz"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kUvilK7BjIg",
        "outputId": "596b278a-bbef-4ec9-a63a-7cbd8c405cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average loss:= 0.3166638671875 - Accuracy:= 0.25358328223228455\n",
            "Number of Epoch = 2 - Average loss:= 0.3108456380208333 - Accuracy:= 0.38259994983673096\n",
            "Number of Epoch = 3 - Average loss:= 0.30740009765625 - Accuracy:= 0.42386671900749207\n",
            "Number of Epoch = 4 - Average loss:= 0.30408190104166666 - Accuracy:= 0.4607832431793213\n",
            "Number of Epoch = 5 - Average loss:= 0.2998575846354167 - Accuracy:= 0.5325832962989807\n",
            "Number of Epoch = 6 - Average loss:= 0.2961735026041667 - Accuracy:= 0.5846498608589172\n",
            "Number of Epoch = 7 - Average loss:= 0.29300641276041667 - Accuracy:= 0.6183501482009888\n",
            "Number of Epoch = 8 - Average loss:= 0.29051422526041665 - Accuracy:= 0.6276001334190369\n",
            "Number of Epoch = 9 - Average loss:= 0.287590234375 - Accuracy:= 0.6532666087150574\n",
            "Number of Epoch = 10 - Average loss:= 0.28512145182291665 - Accuracy:= 0.6630833148956299\n",
            "Number of Epoch = 11 - Average loss:= 0.28214560546875 - Accuracy:= 0.6921166777610779\n",
            "Number of Epoch = 12 - Average loss:= 0.27955813802083335 - Accuracy:= 0.7072334289550781\n",
            "Number of Epoch = 13 - Average loss:= 0.27612294921875 - Accuracy:= 0.7549667358398438\n",
            "Number of Epoch = 14 - Average loss:= 0.2734160807291667 - Accuracy:= 0.7760832905769348\n",
            "Number of Epoch = 15 - Average loss:= 0.270930224609375 - Accuracy:= 0.7897000908851624\n",
            "Number of Epoch = 16 - Average loss:= 0.268696630859375 - Accuracy:= 0.7942834496498108\n",
            "Number of Epoch = 17 - Average loss:= 0.266469921875 - Accuracy:= 0.7993667125701904\n",
            "Number of Epoch = 18 - Average loss:= 0.2642994954427083 - Accuracy:= 0.8028833866119385\n",
            "Number of Epoch = 19 - Average loss:= 0.26220774739583336 - Accuracy:= 0.8041332364082336\n",
            "Number of Epoch = 20 - Average loss:= 0.260040673828125 - Accuracy:= 0.8091000318527222\n",
            "Number of Epoch = 21 - Average loss:= 0.257826806640625 - Accuracy:= 0.8158000707626343\n",
            "Number of Epoch = 22 - Average loss:= 0.2557224772135417 - Accuracy:= 0.8194332122802734\n",
            "Number of Epoch = 23 - Average loss:= 0.2536700846354167 - Accuracy:= 0.8216167688369751\n",
            "Number of Epoch = 24 - Average loss:= 0.251779638671875 - Accuracy:= 0.8183834552764893\n",
            "Number of Epoch = 25 - Average loss:= 0.24963323567708334 - Accuracy:= 0.8250998854637146\n",
            "Number of Epoch = 26 - Average loss:= 0.24778177083333333 - Accuracy:= 0.8214996457099915\n",
            "Number of Epoch = 27 - Average loss:= 0.24578645833333335 - Accuracy:= 0.8239665031433105\n",
            "Number of Epoch = 28 - Average loss:= 0.24371956380208334 - Accuracy:= 0.8294002413749695\n",
            "Number of Epoch = 29 - Average loss:= 0.24185055338541667 - Accuracy:= 0.8281832933425903\n",
            "Number of Epoch = 30 - Average loss:= 0.23983865559895834 - Accuracy:= 0.832883358001709\n",
            "\n",
            "Total time taken (in seconds): 283.39\n"
          ]
        }
      ],
      "source": [
        "mlp = MLP(size_input, size_hidden1, size_hidden2, size_output, device='gpu')\n",
        "time_start = time.time()\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "  acc = 0\n",
        "  count = 0\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(32, seed=epoch*(my_seed)).batch(600)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp.forward(inputs)\n",
        "    loss_total = loss_total + mlp.loss(preds, outputs)\n",
        "    lt = lt + mlp.loss(preds, outputs)\n",
        "    mlp.backward(inputs, outputs)\n",
        "    acc = acc + mlp.accuracy(preds, outputs)\n",
        "    count += 1\n",
        "\n",
        "  loss_list.append(np.sum(loss_total * 100) / X_train.shape[0])\n",
        "  acc_list.append(acc/count*100)\n",
        "  print('Number of Epoch = {} - Average loss:= {} - Accuracy:= {}'.format(epoch + 1, np.sum(loss_total) / X_train.shape[0], acc/count))\n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "# For per epoch_time = Total_Time / Number_of_epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot lines\n",
        "plt.plot(loss_list, label = \"line 1\")\n",
        "plt.plot(acc_list, label = \"line 2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vJm0L7F0OFUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8b0c0a70-3d01-472f-c01a-2e5445ac793d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "#test_loss_total = 0.0\n",
        "acc, n = 0, 0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp.forward(inputs)\n",
        "  #b = mlp_on_default.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp.loss(preds, outputs)\n",
        "  acc = acc + mlp.accuracy(preds, outputs)\n",
        "  n+=1\n",
        "  \n",
        "# a = (test_loss_total.numpy() / X_train.shape[0])\n",
        "# print(X_train.shape[0])\n",
        "# print(test_loss_total.numpy())\n",
        "# print(b)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "print(acc/n)"
      ],
      "metadata": {
        "id": "pBUtS2oPSKne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1a4af9-03bc-4639-9286-6501530ba8e3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7474\n",
            "tf.Tensor(0.83666134, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IST597_HW2_Ajay.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}